#!/usr/bin/env python3
"""
Neural Architecture Search (NAS) –¥–ª—è Food101
–ó–∞–ø—É—Å–∫ –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é Optuna (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ ViT, CNN –∏ –¥—Ä—É–≥–∏—Ö)
"""

import torch
import random
import numpy as np
import optuna
from src.data import get_loaders
from src.models import get_model, ViT, CustomCNN
from src.trainer import train_loop
from src.visualization import generate_plots
from omegaconf import OmegaConf
import logging
import datetime
import os
import argparse
import time
from src.utils import compute_custom_score
import shutil
from optuna.study import Study
from optuna.trial import FrozenTrial


def save_best_config_callback(study: Study, trial: FrozenTrial):
    """Callback to save the best model's config after each trial."""
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –ª—É—á—à–∏–π trial. –≠—Ç–æ –º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å ValueError, –µ—Å–ª–∏ –Ω–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö trial.
        best_trial = study.best_trial
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π trial ‚Äî –ª—É—á—à–∏–π
        if best_trial is not None and trial.number == best_trial.number:
            logger = logging.getLogger(__name__)
            logger.info(f"Trial {trial.number} is the new best trial so far with score {trial.value:.4f}.")
            
            # study.user_attrs —Ç–µ–ø–µ—Ä—å —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è –≤ main()
            experiment_dir = study.user_attrs.get("experiment_dir")
            # –ö–æ–Ω—Ñ–∏–≥ —Ç–µ–ø–µ—Ä—å –±–µ—Ä–µ–º –Ω–∞–ø—Ä—è–º—É—é –∏–∑ user_attrs —Ç–µ–∫—É—â–µ–≥–æ trial
            best_config = trial.user_attrs.get("config")

            if not experiment_dir or not best_config:
                logger.warning("Could not save best config: 'experiment_dir' or 'config' not found in user_attrs.")
                return

            destination_path = os.path.join(experiment_dir, "best_overall_config.yaml")
            
            try:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º dict –æ–±—Ä–∞—Ç–Ω–æ –≤ OmegaConf –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                conf_to_save = OmegaConf.create(best_config)
                OmegaConf.save(config=conf_to_save, f=destination_path)
                logger.info(f"Saved new best config from trial {trial.number} to {destination_path}")
            except Exception as e:
                logger.error(f"Failed to save best config to {destination_path}: {e}")

    except ValueError:
        # –≠—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç, –∫–æ–≥–¥–∞ –µ—â–µ –Ω–µ—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö trials (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—Å–µ –±—ã–ª–∏ pruned)
        logger = logging.getLogger(__name__)
        logger.info(f"Callback for trial {trial.number} skipped: no completed trials yet to determine best trial.")
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"An unexpected error occurred in save_best_config_callback for trial {trial.number}: {e}")


def suggest_params(trial, params_cfg, prefix=""):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é Optuna.
    """
    params = {}
    try:
        for param_name, param_info in params_cfg.items():
            p_type = param_info.get('type')
            full_name = f"{prefix}{param_name}" if prefix else param_name
            if p_type == 'int':
                params[param_name] = trial.suggest_int(full_name, param_info['range'][0], param_info['range'][1])
            elif p_type == 'float':
                params[param_name] = trial.suggest_float(full_name, param_info['range'][0], param_info['range'][1], log=param_info.get('log', False))
            elif p_type == 'categorical':
                params[param_name] = trial.suggest_categorical(full_name, param_info['options'])
            elif p_type.startswith('list_'):
                sub_type = p_type.split('_')[1]
                num_items = param_info['num_blocks']
                range_or_options = param_info.get('range_per_block') or param_info.get('options_per_block', [])
                params[param_name] = []
                for i in range(num_items):
                    item_name = f"{full_name}_{i}"
                    if sub_type == 'int':
                        val = trial.suggest_int(item_name, range_or_options[0], range_or_options[1])
                        params[param_name].append(val)
                    elif sub_type == 'float':
                        val = trial.suggest_float(item_name, range_or_options[0], range_or_options[1], log=param_info.get('log', False))
                        params[param_name].append(val)
                    elif sub_type == 'categorical':
                        val = trial.suggest_categorical(item_name, range_or_options)
                        params[param_name].append(val)
                    else:
                        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø–æ–¥—Ç–∏–ø '{sub_type}' –¥–ª—è list –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ '{param_name}'")
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–∞: {p_type} –¥–ª—è {param_name}")
    except KeyError as e:
        logging.getLogger(__name__).error(f"–û—à–∏–±–∫–∞ –≤ –∫–æ–Ω—Ñ–∏–≥–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á {e}")
        raise
    return params

def objective_architecture_search(trial, cfg):
    """
    Objective —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.
    """
    logger = logging.getLogger(__name__)
    logger.info(f"–ù–∞—á–∞–ª–æ Trial {trial.number} –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.")

    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥–∞
    if not hasattr(cfg.nas.architecture_search, 'model_params'):
        raise ValueError("–ö–æ–Ω—Ñ–∏–≥ NAS –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'nas.architecture_search.model_params'.")

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
    arch_params = suggest_params(trial, cfg.nas.architecture_search.model_params, prefix="arch_")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏
    model_type = cfg.model.custom_model_type if cfg.model.model_type == 'custom' else cfg.model.model_type

    # –°–ø–µ—Ü–∏—Ñ–∏–∫–∞ –¥–ª—è ViT: –∫–æ—Ä—Ä–µ–∫—Ü–∏—è dim –∏ —Ä–∞—Å—á–µ—Ç mlp_dim
    if model_type == 'vit':
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–ª—é—á–µ–π
        if 'heads' not in arch_params or 'dim' not in arch_params or 'mlp_ratio' not in arch_params:
             raise ValueError("–î–ª—è ViT –≤ model_params –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å 'heads', 'dim' –∏ 'mlp_ratio'.")

        desired_heads = arch_params['heads']
        desired_dim = arch_params['dim']
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π dim
        dim = ViT.find_compatible_dim(desired_dim, desired_heads, config=cfg)
        arch_params['dim'] = dim
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º mlp_dim –Ω–∞ –æ—Å–Ω–æ–≤–µ mlp_ratio
        arch_params['mlp_dim'] = int(dim * arch_params['mlp_ratio'])
        # –£–¥–∞–ª—è–µ–º mlp_ratio, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞ ViT
        del arch_params['mlp_ratio']

    print(f"\n   –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Trial {trial.number} (–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, {model_type}):")
    for k, v in arch_params.items():
        print(f"      {k.capitalize()}: {v}")

    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
    for k, v in arch_params.items():
        setattr(cfg.model, k, v)
    
    # –Ø–≤–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    try:
        _ = cfg.train.lr
        _ = cfg.model.dropout
        _ = cfg.train.batch_size
    except Exception as e:
        logger.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (lr, dropout, batch_size) –≤ –∫–æ–Ω—Ñ–∏–≥–µ –¥–ª—è —ç—Ç–∞–ø–∞ –ø–æ–∏—Å–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: {e}")
        raise

    # –£–¥–∞–ª—è–µ–º –∂–µ—Å—Ç–∫–æ –∑–∞–¥–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —Ç–µ–ø–µ—Ä—å –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–æ–Ω—Ñ–∏–≥–µ
    cfg.train.epochs = cfg.nas.architecture_search.epochs_per_trial

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    try:
        model = get_model(cfg)
        model.to(cfg.hardware.device)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ Trial {trial.number}: {e}")
        raise optuna.TrialPruned()

    num_params = model.get_num_params()
    logger.info(f"Trial {trial.number} - Num Parameters: {num_params:,}")
    trial.set_user_attr('num_params', num_params)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
    train_loader, val_loader = get_loaders(cfg)

    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ —ç—Ç–æ–≥–æ trial
    trial_checkpoint_dir = os.path.join(cfg.train.save_dir, f"trial_{trial.number}")
    os.makedirs(trial_checkpoint_dir, exist_ok=True)
    trial.set_user_attr("checkpoint_dir", trial_checkpoint_dir)

    # –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
    checkpoint_dir = os.path.join(cfg.train.save_dir, f"trial_{trial.number}")
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    try:
        best_val_accuracy, avg_time_per_epoch = train_loop(model, train_loader, val_loader, cfg, checkpoint_dir, trial)
    except RuntimeError as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –≤ Trial {trial.number}: {e}")
        raise optuna.TrialPruned()

    trial.set_user_attr('avg_time_per_epoch', avg_time_per_epoch)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é trial
    trial.set_user_attr('config', arch_params)

    # –í—ã—á–∏—Å–ª—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π score
    score = compute_custom_score(best_val_accuracy, num_params, avg_time_per_epoch, cfg)
    logger.info(f"Trial {trial.number} (architecture) - Custom score: {score:.4f}, Accuracy: {best_val_accuracy:.4f}, Params: {num_params:,}, Time per epoch: {avg_time_per_epoch:.2f}s")

    return score

def objective_hyperparameter_search(trial, cfg, architecture_params):
    """
    Objective —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    """
    logger = logging.getLogger(__name__)
    logger.info(f"–ù–∞—á–∞–ª–æ Trial {trial.number} –¥–ª—è –ø–æ–∏—Å–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏
    model_type = cfg.model.custom_model_type if cfg.model.model_type == 'custom' else cfg.model.model_type

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
    for k, v in architecture_params.items():
        setattr(cfg.model, k, v)

    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥–∞
    if not hasattr(cfg.nas.hyperparameters_search, 'hyper_params'):
        raise ValueError("–ö–æ–Ω—Ñ–∏–≥ NAS –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'nas.hyperparameters_search.hyper_params'.")

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    hyp_params = suggest_params(trial, cfg.nas.hyperparameters_search.hyper_params, prefix="hyp_")

    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    cfg.train.lr = hyp_params.get('lr', 1e-4)
    cfg.model.dropout = hyp_params.get('dropout', 0.1)
    cfg.train.weight_decay = hyp_params.get('weight_decay', 0.01)
    cfg.train.batch_size = hyp_params.get('batch_size', 8)
    cfg.train.epochs = cfg.nas.hyperparameters_search.epochs_per_trial

    print(f"\n   –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Trial {trial.number} (–≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, {model_type}):")
    for k, v in hyp_params.items():
        print(f"      {k.capitalize()}: {v}")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    try:
        model = get_model(cfg)
        model.to(cfg.hardware.device)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ Trial {trial.number}: {e}")
        raise optuna.TrialPruned()

    num_params = model.get_num_params()
    logger.info(f"Trial {trial.number} - Num Parameters: {num_params:,}")
    trial.set_user_attr('num_params', num_params)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
    train_loader, val_loader = get_loaders(cfg)

    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ —ç—Ç–æ–≥–æ trial
    trial_checkpoint_dir = os.path.join(cfg.train.save_dir, f"trial_{trial.number}")
    os.makedirs(trial_checkpoint_dir, exist_ok=True)
    trial.set_user_attr("checkpoint_dir", trial_checkpoint_dir)

    # –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
    checkpoint_dir = os.path.join(cfg.train.save_dir, f"trial_{trial.number}")
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    try:
        best_val_accuracy, avg_time_per_epoch = train_loop(model, train_loader, val_loader, cfg, checkpoint_dir, trial)
    except RuntimeError as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –≤ Trial {trial.number}: {e}")
        raise optuna.TrialPruned()

    trial.set_user_attr('avg_time_per_epoch', avg_time_per_epoch)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é trial
    trial_config = architecture_params.copy()
    trial_config.update(hyp_params)
    trial.set_user_attr('config', trial_config)

    # –í—ã—á–∏—Å–ª—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π score
    score = compute_custom_score(best_val_accuracy, num_params, avg_time_per_epoch, cfg)
    logger.info(f"Trial {trial.number} (hyperparameters) - Custom score: {score:.4f}, Accuracy: {best_val_accuracy:.4f}, Params: {num_params:,}, Time per epoch: {avg_time_per_epoch:.2f}s")

    return score

def select_best_model(study, cfg):
    """
    –í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ –∫–∞—Å—Ç–æ–º–Ω–æ–º—É score.
    """
    logger = logging.getLogger(__name__)
    best_trial = None
    best_score = -float('inf')

    for trial in study.trials:
        if trial.state != optuna.trial.TrialState.COMPLETE:
            continue
        score = trial.value  # Value —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–∞—Å—Ç–æ–º–Ω—ã–π score –∏–∑ objective
        if score > best_score:
            best_score = score
            best_trial = trial

    if best_trial is None:
        logger.warning("–ù–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö trials. –í–æ–∑–≤—Ä–∞—â–∞–µ–º None.")
        return None

    logger.info(f"–í—ã–±—Ä–∞–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å: score={best_score:.4f}, config={best_trial.user_attrs['config']}")
    return best_trial

def main(config_path):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ NAS."""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"–ö–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–æ–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–º–µ–Ω–∏
    base_cfg = OmegaConf.load("configs/base.yaml")
    model_cfg = OmegaConf.load(config_path)
    cfg_from_files = OmegaConf.merge(base_cfg, model_cfg)
    model_name_for_log = "unknown_model"
    if cfg_from_files.model.model_type == 'timm':
        model_name_for_log = cfg_from_files.model.timm_model_name
    elif cfg_from_files.model.model_type == 'custom':
        model_name_for_log = f"custom_{cfg_from_files.model.custom_model_type}"

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    log_file = f"log_{timestamp}_nas_{model_name_for_log}.log"
    log_path = os.path.join(log_dir, log_file)
    
    logging.basicConfig(
        filename=log_path,
        filemode='w',
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        level=logging.INFO
    )
    logging.captureWarnings(True)
    logger = logging.getLogger(__name__)

    # --- –õ–æ–≥–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è/–≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è study ---
    optuna_storage_dir = "optuna_studies"
    os.makedirs(optuna_storage_dir, exist_ok=True)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ study –≤ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–º –∫–æ–Ω—Ñ–∏–≥–µ
    if hasattr(cfg_from_files.nas, 'study_name') and hasattr(cfg_from_files.nas, 'study_db_path'):
        # --- –ü–£–¢–¨ –í–û–ó–û–ë–ù–û–í–õ–ï–ù–ò–Ø ---
        study_name = cfg_from_files.nas.study_name
        study_db_file = cfg_from_files.nas.study_db_path
        cfg = cfg_from_files
        logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –∫–æ–Ω—Ñ–∏–≥–µ. –í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ study '{study_name}' –∏–∑ —Ñ–∞–π–ª–∞ {study_db_file}")
    else:
        # --- –ü–£–¢–¨ –°–û–ó–î–ê–ù–ò–Ø –ù–û–í–û–ì–û STUDY ---
        study_name = f"{os.path.basename(config_path).replace('.yaml', '')}_{model_name_for_log}"
        study_config_path = os.path.join(optuna_storage_dir, f'{study_name}_config.yaml')

        if os.path.exists(study_config_path):
            raise FileExistsError(f"Study —Å –∏–º–µ–Ω–µ–º '{study_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. "
                                  f"–ß—Ç–æ–±—ã –≤–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å –µ–≥–æ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥: "
                                  f"python nas.py --config {study_config_path}")
        
        logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ study '{study_name}'.")
        cfg = cfg_from_files
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—É—Ç—å –∫ –ë–î –∏ –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –∏ –∏–º—è study –≤ –∫–æ–Ω—Ñ–∏–≥
        study_db_path = os.path.join(optuna_storage_dir, f'{study_name}.db')
        study_db_file = f"sqlite:///{study_db_path}"
        cfg.nas.study_name = study_name
        cfg.nas.study_db_path = study_db_file

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –±—É–¥—É—â–∏—Ö –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤ {study_config_path}")
        OmegaConf.save(cfg, study_config_path)

    # –§–∏–∫—Å–∞—Ü–∏—è seed
    if cfg.train.manual_seed_enabled:
        SEED = cfg.train.seed
        torch.manual_seed(SEED)
        random.seed(SEED)
        np.random.seed(SEED)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(SEED)
    
    # –°–æ–∑–¥–∞—ë–º –æ—Å–Ω–æ–≤–Ω—É—é –ø–∞–ø–∫—É –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (—Å timestamp)
    experiment_name = f"{timestamp}_nas_{model_name_for_log}"
    experiment_dir = os.path.join("checkpoints", experiment_name)
    os.makedirs(experiment_dir, exist_ok=True)
    cfg.train.save_dir = experiment_dir

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –∞—Ä—Ö–∏–≤–∞ —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
    cfg.nas.study_db_path = study_db_file
    config_path_saved = os.path.join(experiment_dir, "nas_config.yaml")
    with open(config_path_saved, 'w') as f:
        OmegaConf.save(cfg, f)
    
    logger.info("Starting NAS experiment with config saved at: %s", config_path_saved)
    logger.info(f"Experiment directory: {experiment_dir}")
    logger.info(f"Optuna study results will be saved to: {study_db_file}")

    logger.info("\n" + "="*60 + "\nüìö –≠–¢–ê–ü 1: –ü–æ–∏—Å–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã\n" + "="*60)

    # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —ç—Ç–æ–≥–æ —ç—Ç–∞–ø–∞
    try:
        logger.info("--- –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã ---")
        logger.info(f"Learning Rate: {cfg.train.lr}")
        logger.info(f"Dropout: {cfg.model.dropout}")
        logger.info(f"Batch Size: {cfg.train.batch_size}")
        logger.info("----------------------------------------------------")
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞: {e}")
        raise

    # -- –≠–¢–ê–ü 1: –ü–æ–∏—Å–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã --

    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è sampler –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    arch_search_cfg = cfg.nas.architecture_search
    sampler_params_arch = arch_search_cfg.get('sampler_params', {})
    pruner_params_arch = arch_search_cfg.get('pruner_params', {})

    try:
        sampler_arch = getattr(optuna.samplers, arch_search_cfg.sampler)(**sampler_params_arch)
        pruner_arch = getattr(optuna.pruners, arch_search_cfg.pruner)(**pruner_params_arch)
        logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º Sampler: {arch_search_cfg.sampler}, Pruner: {arch_search_cfg.pruner}")
    except AttributeError as e:
        logger.error(f"–ù–µ–≤–µ—Ä–Ω–æ–µ –∏–º—è Sampler –∏–ª–∏ Pruner –≤ –∫–æ–Ω—Ñ–∏–≥–µ: {e}")
        raise

    study_params_arch = {
        "storage": study_db_file,
        "study_name": f"{study_name}_architecture",
        "direction": arch_search_cfg.direction,
        "sampler": sampler_arch,
        "pruner": pruner_arch,
        "load_if_exists": True
    }
    
    study_arch = optuna.create_study(**study_params_arch)
    study_arch.set_user_attr("config", OmegaConf.to_container(cfg, resolve=True))
    study_arch.optimize(
        lambda trial: objective_architecture_search(trial, cfg),
        n_trials=arch_search_cfg.num_trials,
        callbacks=[save_best_config_callback]
    )

    try:
        best_trial_arch = study_arch.best_trial
        logger.info(f"–ü–æ–∏—Å–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∑–∞–≤–µ—Ä—à–µ–Ω. –õ—É—á—à–∏–π score: {best_trial_arch.value:.4f}")
        best_architecture_params = {k.replace('arch_', ''): v for k, v in best_trial_arch.params.items()}
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏–∑ –ª—É—á—à–µ–≥–æ trial –¥–ª—è –ø–æ–∏—Å–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {best_architecture_params}")
    except ValueError:
        logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–≥–æ trial –≤ –ø–æ–∏—Å–∫–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –ø–æ–∏—Å–∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.")
        logger.error("–ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–µ–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ–∏—Å–∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —É–∂–µ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.")
        return  # –ó–∞–≤–µ—Ä—à–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, –µ—Å–ª–∏ –Ω–µ –∏–∑ —á–µ–≥–æ –≤—ã–±—Ä–∞—Ç—å

    logger.info("\n" + "="*60 + "\n–≠–¢–ê–ü 2: –ü–æ–∏—Å–∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n" + "="*60)

    # -- –≠–¢–ê–ü 2: –ü–æ–∏—Å–∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ --

    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ —ç—Ç–∞–ø–∞
    hp_search_cfg = cfg.nas.hyperparameters_search
    sampler_params_hp = hp_search_cfg.get('sampler_params', {})
    pruner_params_hp = hp_search_cfg.get('pruner_params', {})
    
    try:
        sampler_hp = getattr(optuna.samplers, hp_search_cfg.sampler)(**sampler_params_hp)
        pruner_hp = getattr(optuna.pruners, hp_search_cfg.pruner)(**pruner_params_hp)
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º Sampler: {hp_search_cfg.sampler}, Pruner: {hp_search_cfg.pruner}")
    except AttributeError as e:
        logger.error(f"–ù–µ–≤–µ—Ä–Ω–æ–µ –∏–º—è Sampler –∏–ª–∏ Pruner –≤ –∫–æ–Ω—Ñ–∏–≥–µ: {e}")
        raise
        
    study_params_hp = {
        "storage": study_db_file,
        "study_name": f"{study_name}_hyperparameters",
        "direction": hp_search_cfg.direction,
        "sampler": sampler_hp,
        "pruner": pruner_hp,
        "load_if_exists": True
    }

    study_hp = optuna.create_study(**study_params_hp)
    study_hp.set_user_attr("config", OmegaConf.to_container(cfg, resolve=True))
    study_hp.optimize(
        lambda trial: objective_hyperparameter_search(trial, cfg, best_architecture_params),
        n_trials=hp_search_cfg.num_trials,
        callbacks=[save_best_config_callback]
    )

    best_hyp_trial = select_best_model(study_hp, cfg)
    if best_hyp_trial:
        best_hyperparameters = best_hyp_trial.user_attrs.get('config', {})
        logger.info(f"–õ—É—á—à–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_hyperparameters} —Å score {best_hyp_trial.value:.4f}")
        print("\n–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
        print(f"–õ—É—á—à–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_hyperparameters}")
        print(f"   Score: {best_hyp_trial.value:.4f}")

    logger.info("NAS experiment completed.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run Neural Architecture Search for Food101.")
    parser.add_argument("--config", type=str, default="configs/nas_vit.yaml",
                        help="Path to the NAS configuration file.")
    args = parser.parse_args()
    main(args.config)
    